{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425a48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torchdata\n",
    "import portalocker\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_STATE = 30255\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc609998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Renewable Energy Sources                       8117\n",
       "Geosciences                                     142\n",
       "Environmental Sciences                           98\n",
       "Energy Storage, Conversion, and Utilization      55\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/preprocessed_data.csv')\n",
    "df = df[['CLASS', 'PREPROCESSED']]\n",
    "df = df.dropna()\n",
    "df['PREPROCESSED'] = df['PREPROCESSED'].str.replace(r'<[^<>]*>', '', regex=True) # drop HTML tags\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['CLASS'])\n",
    "df['LABEL'] = le.transform(df['CLASS'])\n",
    "df.head()\n",
    "\n",
    "display(df['CLASS'].value_counts())\n",
    "df = df[['LABEL', 'PREPROCESSED']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc5ca62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    8117\n",
       "2     142\n",
       "1      98\n",
       "0      55\n",
       "Name: LABEL, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['LABEL'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c580e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(df, random_state):\n",
    "    \n",
    "    # Split the data into training, testing, and validation sets\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=random_state)\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=random_state)\n",
    "\n",
    "    # Convert the sets into iterable\n",
    "    train_iter = iter(train_data.values.tolist())\n",
    "    test_data = iter(test_data.values.tolist())\n",
    "    val_data = iter(val_data.values.tolist())\n",
    "    \n",
    "    return train_iter, test_data, val_data\n",
    "\n",
    "train_iter, test_data, val_data = split_data(df, RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd091518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(train_iter):\n",
    "    for _, text in train_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "    \n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_iter), specials=[\"<unk>\"], min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e577761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: The number of words in the Vocab object is 4666.\n",
      "Task 2: The index of the word 'energy' is 4.\n",
      "Task 3: The word at index 500 is 'strong'.\n",
      "Task 4: The index of the word '<unk>' is 0. Resetting default index to this value.\n"
     ]
    }
   ],
   "source": [
    "# Task 1\n",
    "print(f\"Task 1: The number of words in the Vocab object is {len(vocab)}.\")\n",
    "\n",
    "# # Task 2\n",
    "stoi_dict = vocab.get_stoi()\n",
    "word = \"energy\"\n",
    "print(f\"Task 2: The index of the word '{word}' is {stoi_dict[word]}.\")\n",
    "\n",
    "# # Task 3\n",
    "itos_dict = vocab.get_itos()\n",
    "idx = 500\n",
    "print(f\"Task 3: The word at index 500 is '{itos_dict[idx]}'.\")\n",
    "\n",
    "# # Task 4:\n",
    "word = \"<unk>\"\n",
    "print(f\"Task 4: The index of the word '{word}' is {stoi_dict[word]}. Resetting default index to this value.\")\n",
    "vocab.set_default_index(stoi_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bfc7112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "def collate_into_bow(batch):\n",
    "    '''\n",
    "    Generates a tensor of batch labels and a tensor of relative token frequencies.\n",
    "    \n",
    "    arg:\n",
    "    - batch: List of tuples, first element of tuple\n",
    "        is a label, second element is text\n",
    "    - assumes that Vocab object is created\n",
    "    tr\n",
    "    Returns:\n",
    "    - Tensor (1D; same length as batch) showing text labels (indexed to 0)\n",
    "    - Tensor (2D; rows are the length of batch, columns are length of Vocab object)\n",
    "        showing the relative frequency of each token within the text\n",
    "    '''\n",
    "    # get tensor dimensions\n",
    "    k = len(batch)\n",
    "    m = len(vocab)\n",
    "    \n",
    "    # initialize empty tensors\n",
    "    tensor_labels = torch.zeros((k, ), dtype=torch.int64)\n",
    "    tensor_rf = torch.zeros((k, m))\n",
    "\n",
    "    # iterate over batch\n",
    "    for idx, (label, txt) in enumerate(batch):\n",
    "\n",
    "        # get individual tokens\n",
    "        txt_split = txt.split(\" \")\n",
    "\n",
    "        # get indices for each token\n",
    "        txt_indices = vocab.lookup_indices(txt_split)\n",
    "\n",
    "        # get frequencies for eacch token\n",
    "        idx_freq_dict = dict(Counter(txt_indices))\n",
    "\n",
    "        # update tensor with frequency of each token\n",
    "        tensor_rf[idx, list(idx_freq_dict.keys())] += torch.tensor(list(idx_freq_dict.values()))\n",
    "\n",
    "    # normalize so that rows sum to 1\n",
    "    tensor_row_sum = tensor_rf.sum(dim=1, keepdim=True)\n",
    "    tensor_rf = tensor_rf / tensor_row_sum\n",
    "    \n",
    "    return tensor_labels, tensor_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "448512d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class MyIterableDataset(IterableDataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # Return an iterator over your data\n",
    "        return iter(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98afe0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([16]) torch.Size([16, 4666])\n",
      "1 torch.Size([16]) torch.Size([16, 4666])\n",
      "2 torch.Size([16]) torch.Size([16, 4666])\n",
      "3 torch.Size([16]) torch.Size([16, 4666])\n",
      "4 torch.Size([16]) torch.Size([16, 4666])\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_data, val_data = split_data(df, RANDOM_STATE)\n",
    "dataloader = DataLoader(MyIterableDataset(train_iter), batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        collate_fn=collate_into_bow)\n",
    "for idx, (lt, tt) in enumerate(dataloader):\n",
    "    print(idx, lt.shape, tt.shape)\n",
    "    if idx == 4: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "899c97d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a BoWClassifier class with one single linear layer\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        \n",
    "        # create affine map\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "        # single linear layer\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71ce4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _, _ = split_data(df, RANDOM_STATE)\n",
    "num_labels = len(set([label for (label, text) in train_data]))\n",
    "vocab_size = len(vocab)\n",
    "model = BoWClassifier(num_labels, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4242112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "\n",
    "def train_an_epoch(dataloader, optimizer):\n",
    "    model.train() # Sets the module in training mode.\n",
    "    log_interval = 500\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        model.zero_grad()\n",
    "        log_probs = model(text)\n",
    "        loss = loss_function(log_probs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(f'At iteration {idx} the loss is {loss:.3f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c62b1d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to compute accuracy\n",
    "\n",
    "def get_accuracy(dataloader):\n",
    "    '''\n",
    "    Compute accuracy rate of model. Generate\n",
    "    model predictions, compare to true labels,\n",
    "    and compute accuracy.\n",
    "    \n",
    "    args:\n",
    "    - dataloader (object)\n",
    "    \n",
    "    Returns: An accuracy rate (float)\n",
    "    '''\n",
    "    \n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # initialize counters\n",
    "    correct_count = 0.0\n",
    "    example_count = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        # unpack dataloader\n",
    "        for tl, tt in dataloader:\n",
    "            \n",
    "            # get the size of the batch\n",
    "            example_count += tl.shape[0]\n",
    "\n",
    "            # get predicted values (label with highest probability)\n",
    "            model_result = model(tt)\n",
    "            tensor_pred = model_result.argmax(dim=1) \n",
    "\n",
    "            # count how often predictions match true labels\n",
    "            correct_count_batch = (tensor_pred == tl).sum().item()\n",
    "            correct_count += correct_count_batch\n",
    "            \n",
    "            i += 1\n",
    "    \n",
    "    if example_count == 0:\n",
    "        print(\"correct_count:\", correct_count, \"iter number:\", i)\n",
    "    return correct_count / example_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "363b8e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter, test_data, val_data = split_data(df, RANDOM_STATE)\n",
    "# train_dataloader = DataLoader(MyIterableDataset(train_iter), batch_size=BATCH_SIZE, shuffle=False, \n",
    "#                         collate_fn=collate_into_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7517e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, test_data, val_data = split_data(df, RANDOM_STATE)\n",
    "\n",
    "# train_dataloader = DataLoader(MyIterableDataset(train_data), batch_size=BATCH_SIZE, \n",
    "#                               collate_fn=collate_into_bow)\n",
    "# valid_dataloader = DataLoader(MyIterableDataset(val_data), batch_size=BATCH_SIZE, \n",
    "#                               collate_fn=collate_into_bow)\n",
    "# test_dataloader = DataLoader(MyIterableDataset(test_data), batch_size=BATCH_SIZE,\n",
    "#                              collate_fn=collate_into_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b486e8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     11\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 12\u001b[0m     train_an_epoch(\u001b[43mtrain_dataloader\u001b[49m, optimizer)\n\u001b[0;32m     13\u001b[0m     train_accuracy \u001b[38;5;241m=\u001b[39m get_accuracy(train_dataloader) \u001b[38;5;66;03m# added\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     train_accuracies\u001b[38;5;241m.\u001b[39mappend(train_accuracy)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "EPOCHS = 40 # epoch\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=3)\n",
    "\n",
    "train_accuracies=[] # added\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_an_epoch(train_dataloader, optimizer)\n",
    "    train_accuracy = get_accuracy(train_dataloader) # added\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    time_taken = time.time() - epoch_start_time\n",
    "    print()\n",
    "    print(f'After epoch {epoch} the train accuracy is {train_accuracy:.3f}.')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95807428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TRAINING\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# EPOCHS = 40 # epoch\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=3)\n",
    "\n",
    "# accuracies=[]\n",
    "# train_accuracies=[] # added\n",
    "# test_accuracies=[] # added\n",
    "# for epoch in range(1, EPOCHS + 1):\n",
    "#     epoch_start_time = time.time()\n",
    "#     train_an_epoch(train_dataloader, optimizer)\n",
    "#     accuracy = get_accuracy(valid_dataloader)\n",
    "#     train_accuracy = get_accuracy(train_dataloader) # added\n",
    "#     test_accuracy = get_accuracy(test_dataloader) # added\n",
    "#     accuracies.append(accuracy)\n",
    "#     train_accuracies.append(train_accuracy)\n",
    "#     test_accuracies.append(test_accuracy)\n",
    "#     time_taken = time.time() - epoch_start_time\n",
    "#     print()\n",
    "#     print(f'After epoch {epoch} the validation accuracy is {train_accuracy:.3f}.')\n",
    "#     print(f'After epoch {epoch} the validation accuracy is {accuracy:.3f}.')\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capp30255",
   "language": "python",
   "name": "capp30255"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
