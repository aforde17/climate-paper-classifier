{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfBsGAe561vd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "if USE_CUDA:\n",
        "    DEVICE = torch.device('cuda')\n",
        "    print(\"Using cuda.\")\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "    print(\"Using cpu.\")\n",
        "\n",
        "seed = 30255    \n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if USE_CUDA:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "# For uploading data to Colab see, e.g., \n",
        "# https://medium.com/@philipplies/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041    \n",
        "\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "PATH = \"gdrive/MyDrive/climate-paper-classifier/data\"\n",
        "    \n"
      ],
      "metadata": {
        "id": "x5Wxa2RM68pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
        "from transformers import BertTokenizer\n",
        "import os\n",
        "\n",
        "path = os.path.abspath(os.path.join(PATH, 'preprocessed_data.csv'))\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "seed = 30255    \n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "class Processing():\n",
        "    def __init__(self, seq_len=512, batch_size=32):\n",
        "        self.df = pd.DataFrame()\n",
        "        self.train_loader = None\n",
        "        self.val_loader = None\n",
        "        self.test_loader = None\n",
        "        self.vocab = pd.DataFrame()\n",
        "        self.seq_len = seq_len\n",
        "        self.batch_size = batch_size\n",
        "        self.num_words = 0\n",
        "\n",
        "    def load(self):\n",
        "        from sklearn import preprocessing\n",
        "        self.df = pd.read_csv(path)\n",
        "        self.df = self.df[['CLASS', 'SPACY_PREPROCESSED']]\n",
        "        \n",
        "        self.df = self.df.dropna()\n",
        "        self.df['SPACY_PREPROCESSED'] = self.df['SPACY_PREPROCESSED'].str.replace(r'<[^<>]*>', '', regex=True) # drop HTML tags\n",
        "\n",
        "        le = preprocessing.LabelEncoder()\n",
        "        le.fit(self.df['CLASS'])\n",
        "        self.df['LABEL'] = le.transform(self.df['CLASS'])\n",
        "\n",
        "        self.df = self.df[['LABEL', 'SPACY_PREPROCESSED']]\n",
        "\n",
        "        self.df['LABEL'].value_counts(dropna=False)\n",
        "        \n",
        "\n",
        "        self.text = self.df[\"SPACY_PREPROCESSED\"].values\n",
        "        self.target = self.df[\"LABEL\"].values\n",
        "    \n",
        "    def tokenize_and_build_vocabulary(self):\n",
        "        from torchtext.data.utils import get_tokenizer\n",
        "        from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "        tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "        def yield_tokens(train_iter):\n",
        "            for text in train_iter:\n",
        "                yield tokenizer(text)\n",
        "\n",
        "        # tokenize and build vocab over all words \n",
        "        train_iter = iter(self.text.tolist())\n",
        "        self.tokenized = list(map(lambda text : tokenizer(text), train_iter))\n",
        "        # re-initialize train iter\n",
        "        train_iter = iter(self.text.tolist())\n",
        "        # build vocab\n",
        "        self.vocab = build_vocab_from_iterator(\n",
        "            yield_tokens(train_iter), specials=[\"<unk>\"], max_tokens = self.seq_len)\n",
        "        self.vocab.set_default_index(self.vocab['<unk>'])\n",
        "        # set num words in vocab\n",
        "        self.num_words = len(self.vocab)\n",
        "\n",
        "    def word_to_idx(self):\n",
        "        # Index representation\t\n",
        "        self.index_representation = list()\n",
        "        for sentence in self.tokenized:\n",
        "            temp_sentence = list()\n",
        "            for word in sentence:\n",
        "                idx = self.vocab.lookup_indices([word])\n",
        "                temp_sentence.extend(self.vocab.lookup_indices([word]))\n",
        "            self.index_representation.append(temp_sentence)\n",
        "\n",
        "    def padding_sentences(self):\n",
        "        # Pad each sentence which does not fulfill the required len\n",
        "        # Zero padding\n",
        "\n",
        "        pad_idx = 0\n",
        "        self.padded = list()\n",
        "        for sentence in self.index_representation: # tensors\n",
        "            if len(sentence) < self.seq_len:\n",
        "                while len(sentence) < self.seq_len: #max_length\n",
        "                    sentence.append(pad_idx)\n",
        "                sentence = torch.tensor(sentence)\n",
        "                self.padded.append(sentence)\n",
        "            else:\n",
        "                sentence = torch.tensor(sentence[:self.seq_len])\n",
        "                self.padded.append(sentence) # new code\n",
        "        self.padded = torch.stack(self.padded)\n",
        "\n",
        "    def split_data(self):\n",
        "        self.target = torch.tensor(self.target)\n",
        "        # Concatenating the Padded Vectors, Labels\n",
        "        dataset = TensorDataset(self.padded, self.target)\n",
        "\n",
        "        # compute train/validation/test split sizes\n",
        "        train_size = int(0.7 * len(dataset))\n",
        "        val_size = int(0.15 * len(dataset))\n",
        "        test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "        # split dataset randomly into train/validation/test sets\n",
        "        train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "        # create data loaders for each set\n",
        "        self.train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
        "        self.val_loader = DataLoader(val_data, batch_size=self.batch_size, shuffle=True)\n",
        "        self.test_loader = DataLoader(test_data, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    def process_all(self):\n",
        "        self.load()\n",
        "        self.tokenize_and_build_vocabulary()\n",
        "        self.word_to_idx()\n",
        "        self.padding_sentences()\n",
        "        self.split_data()\n",
        "\n",
        "    def prints(self):\n",
        "        # Task 1\n",
        "        print(f\"Task 1: The number of words in the Vocab object is {len(self.vocab)}.\")\n",
        "\n",
        "        # # Task 2\n",
        "        stoi_dict = self.vocab.get_stoi()\n",
        "        word = \"energy\"\n",
        "        print(f\"Task 2: The index of the word '{word}' is {stoi_dict[word]}.\")\n",
        "\n",
        "        # # Task 3\n",
        "        itos_dict = self.vocab.get_itos()\n",
        "        idx = 500\n",
        "        print(f\"Task 3: The word at index 500 is '{itos_dict[idx]}'.\")\n",
        "\n",
        "        # # Task 4:\n",
        "        word = \"<unk>\"\n",
        "        print(f\"Task 4: The index of the word '{word}' is {stoi_dict[word]}. Resetting default index to this value.\")\n"
      ],
      "metadata": {
        "id": "sUMWSfsQ7QOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Processing()\n",
        "data.load()"
      ],
      "metadata": {
        "id": "zbaN-PzCPopb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4MimFlA61vh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# source: https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-conv1d-for-text-classification\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, use_glove=True, kernel_size=5, channel_size=32, seq_len=512, batch_size=32):\n",
        "        super(CNN, self).__init__()\n",
        "        self.data = Processing(seq_len, batch_size)\n",
        "        self.data.process_all()\n",
        "        self.use_glove = use_glove\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=len(self.data.vocab), embedding_dim=128)\n",
        "        if use_glove:\n",
        "            self.embedding_pretrained = nn.Embedding.from_pretrained(glove_vectors, freeze=False)\n",
        "            self.conv1 = nn.Conv1d(300, channel_size, kernel_size=kernel_size, padding=\"same\")\n",
        "            self.conv2 = nn.Conv1d(channel_size, channel_size, kernel_size=kernel_size, padding=\"same\")\n",
        "        else:\n",
        "          self.conv1 = nn.Conv1d(128, channel_size, kernel_size=kernel_size, padding=\"same\")\n",
        "          self.conv2 = nn.Conv1d(channel_size, channel_size, kernel_size=kernel_size, padding=\"same\")\n",
        "\n",
        "        self.pooling1 = nn.MaxPool1d(2)\n",
        "        self.pooling2 = nn.MaxPool1d(2)\n",
        "        self.linear = nn.Linear(channel_size, 5)\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        if self.use_glove:\n",
        "          x = self.embedding_pretrained(X_batch)\n",
        "          x = x.transpose(1, 2)\n",
        "          x = nn.functional.relu(self.conv1(x))\n",
        "          x = self.pooling1(x)\n",
        "          x = nn.functional.relu(self.conv2(x))\n",
        "          x = self.pooling2(x)\n",
        "          x, _ = x.max(dim=-1)\n",
        "          x = self.linear(x)\n",
        "          return x\n",
        "        else:\n",
        "          x = self.embedding_layer(X_batch)\n",
        "          # Transpose the tensor to shape [16, 128, 930]\n",
        "          x = x.transpose(1, 2)\n",
        "          x = nn.functional.relu(self.conv1(x))\n",
        "          x = self.pooling1(x)\n",
        "          x = nn.functional.relu(self.conv2(x))\n",
        "          x = self.pooling2(x)\n",
        "          x, _ = x.max(dim=-1)\n",
        "          x = self.linear(x)\n",
        "          return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!pip install torchtext\n",
        "import torchtext\n",
        "\n",
        "data = Processing()\n",
        "data.process_all()\n",
        "\n",
        "cache_dir: str = '~/.vector_cache'\n",
        "glove = torchtext.vocab.GloVe('6B', cache=cache_dir)\n",
        "\n",
        "print(f\"Glove embedding files:\\\n",
        "    {os.listdir(cache_dir)} in {os.path.abspath(cache_dir)}\")\n",
        "\n",
        "glove_vectors = glove.get_vecs_by_tokens(data.vocab.get_itos())\n",
        "print(f\"Glove vectors tensor shape: {glove_vectors.shape}\")"
      ],
      "metadata": {
        "id": "btpeL0mv_DCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk6GvF2861vi"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def CalcValLossAndAccuracy(model, loss_fn, val_loader):\n",
        "    with torch.no_grad():\n",
        "        Y_shuffled, Y_preds, losses = [],[],[]\n",
        "        for X, Y in val_loader:\n",
        "            preds = model(X)\n",
        "            loss = loss_fn(preds, Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            Y_shuffled.append(Y)\n",
        "            Y_preds.append(preds.argmax(dim=-1))\n",
        "\n",
        "        Y_shuffled = torch.cat(Y_shuffled)\n",
        "        Y_preds = torch.cat(Y_preds)\n",
        "\n",
        "        # print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        # print(\"Valid Acc  : {:.3f}\\n\".format(accuracy_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))\n",
        "        return losses, accuracy_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hWF7Hs361vj"
      },
      "outputs": [],
      "source": [
        "seed = 30255    \n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def train(use_glove=True, kernel_size=5, channel_size=32, \n",
        "          seq_len=512, batch_size=32, plot_title=\"\"):\n",
        "  # Model      \n",
        "  model = CNN(use_glove=use_glove, kernel_size=kernel_size, \n",
        "              channel_size=channel_size, seq_len=seq_len, \n",
        "              batch_size=batch_size)\n",
        "  # Opmization function\n",
        "  optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "  # Loss function\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  # Data\n",
        "  data = model.data\n",
        "  loader_train = data.train_loader\n",
        "  loader_val = data.val_loader\n",
        "  loader_test = data.test_loader\n",
        "\n",
        "  for X, Y in loader_val:\n",
        "      print(X.shape, Y.shape)\n",
        "      break\n",
        "\n",
        "  epochs = 10\n",
        "\n",
        "  all_losses = []\n",
        "  all_acc = []\n",
        "  best_model = None\n",
        "\n",
        "  for i in range(epochs):\n",
        "      losses = []\n",
        "      for k, (X, Y) in enumerate(loader_train):\n",
        "          Y_preds = model(X)\n",
        "          loss = loss_fn(Y_preds, Y)\n",
        "          losses.append(loss.item())\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          clip_grad_norm_(model.parameters(), 1)\n",
        "\n",
        "          step = (k+1)\n",
        "          if step % 100 == 0:\n",
        "              loss, accuracy = CalcValLossAndAccuracy(model, loss_fn, loader_val)\n",
        "\n",
        "              all_acc.append(accuracy)\n",
        "              print(\"Epoch\", i, \"Step\", step, \"Train Loss: {:.3f}\".format(torch.tensor(losses).mean()), \n",
        "                    \"Valid Loss: {:.3f}\".format(torch.tensor(loss).mean()),\n",
        "                    \"Valid Acc: {:.3f}\".format(accuracy))\n",
        "              if len(all_losses) > 0:\n",
        "                if torch.tensor(loss).mean() > all_losses[-1]:\n",
        "                  model_copy = type(model)(use_glove=use_glove, \n",
        "                                           kernel_size=kernel_size, \n",
        "                                           channel_size=channel_size, \n",
        "                                           seq_len=seq_len, \n",
        "                                           batch_size=batch_size)\n",
        "                  model_copy.load_state_dict(model.state_dict()) # copy weights and stuff\n",
        "                  best_model = model_copy\n",
        "              else:\n",
        "                all_losses.append(torch.tensor(loss).mean())\n",
        "                model_copy = type(model)(use_glove=use_glove, \n",
        "                                         kernel_size=kernel_size, \n",
        "                                         channel_size=channel_size, \n",
        "                                         seq_len=seq_len, \n",
        "                                         batch_size=batch_size)\n",
        "                model_copy.load_state_dict(model.state_dict()) # copy weights and stuff\n",
        "                best_model = model_copy\n",
        "\n",
        "  caption = \"kernel_size=5, channel_size=32, seq_len=512, batch_size=16\"\n",
        "  plt.plot(range(1, (2*epochs + 1)), all_acc)\n",
        "  plt.title(plot_title)\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Validation Accuracy\")\n",
        "  plt.figtext(0.5, 0.001, caption, wrap=True, horizontalalignment='center', fontsize=8)\n",
        "  plt.show()\n",
        "\n",
        "  return best_model, loader_test\n",
        "\n",
        "kernel_size=5\n",
        "channel_size=32\n",
        "seq_len=512\n",
        "batch_size=16\n",
        "plot_title = \"CNN: Validation Accuracy vs. Epoch, with GloVe\"\n",
        "\n",
        "best_model, loader_test = train(use_glove=True, kernel_size=kernel_size, \n",
        "                                channel_size=channel_size, \n",
        "                                seq_len=seq_len, \n",
        "                                batch_size=batch_size,\n",
        "                                plot_title=plot_title)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhlHfsug61vk"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "def MakePredictions(model, loader):\n",
        "    Y_shuffled, Y_preds = [], []\n",
        "    for X, Y in loader:\n",
        "        preds = model(X)\n",
        "        Y_preds.append(preds)\n",
        "        Y_shuffled.append(Y)\n",
        "    gc.collect()\n",
        "    Y_preds, Y_shuffled = torch.cat(Y_preds), torch.cat(Y_shuffled)\n",
        "\n",
        "    return Y_shuffled.detach().numpy(), nn.functional.softmax(Y_preds, dim=-1).argmax(dim=-1).detach().numpy()\n",
        "\n",
        "Y_actual, Y_preds = MakePredictions(best_model, loader_test)\n",
        "!pip install sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "labels = [\"Energy Storage, Conversion, and Utilization\",\"Environmental Sciences\",\n",
        "          \"Fission and Nuclear Technologies\",\"Fossil Fuels\",\"Renewable Energy Sources\"]\n",
        "f1 = f1_score(Y_actual, Y_preds, average=None, labels=[0,1,2,3,4])\n",
        "logging.info(f\"Best Model F1 Score={f1}\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "print(classification_report(Y_actual, Y_preds, target_names=labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "df = pd.read_csv(path)\n",
        "df = df[['CLASS', 'SPACY_PREPROCESSED']]\n",
        "df = df.dropna()\n",
        "df['SPACY_PREPROCESSED'] = df['SPACY_PREPROCESSED'].str.replace(r'<[^<>]*>', '', regex=True) # drop HTML tags\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(df['CLASS'])\n",
        "df['LABEL'] = le.transform(df['CLASS'])\n",
        "df['LABEL'].value_counts(dropna=False)\n",
        "\n",
        "tmp_dict = df[['CLASS', 'LABEL']].drop_duplicates().set_index('LABEL').to_dict('index')\n",
        "CATEGORY_DICT = {label: sub_dict['CLASS'] for label, sub_dict in tmp_dict.items()}\n",
        "list(CATEGORY_DICT.values())\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.axis import XAxis as ax\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(Y_actual, Y_preds)\n",
        "cm = (cm / cm.sum(axis=1)) * 100\n",
        "\n",
        "# Create a heatmap \n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=list(CATEGORY_DICT.values()), yticklabels=list(CATEGORY_DICT.values()))\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('CNN Confusion Matrix')\n",
        "plt.tick_params(direction='inout')\n",
        "plt.xticks(rotation=60, horizontalalignment='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HZvqN1oV3986"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "capp30255",
      "language": "python",
      "name": "capp30255"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}